{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZI2bHmId4aMM"
   },
   "source": [
    "# Analyzing a Tennis Serve with Machine Learning\n",
    "\n",
    "In this notebook you use the Video Intelligence API to analyze a tennis serve, including the angle of the arms and legs during the serve, and a pretrained custom model to track the speed of a tennis ball after being served."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5cKLmgBFT4Np"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FxR_vKOLRgZ8"
   },
   "source": [
    "First, install the necessary libraries. The first cell will download and install the libraries onto the notebook, and the next cell imports them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GEzSnfTuRnjg"
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-automl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSEyKO4BR6ho"
   },
   "source": [
    "You might have to restart your runtime to load these packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hQUBRTxSb23W"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from google.cloud import automl\n",
    "from google.cloud import videointelligence_v1p3beta1 as videointelligence\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kcCkQaESSn0j"
   },
   "source": [
    "Fill in your info below by specifying your project id and bucket name. Replace YOUR_PROJECT_ID and YOUR_BUCKET_NAME with your Project ID as it's always a unique name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "GywERSiHbMXJ"
   },
   "outputs": [],
   "source": [
    "project_id = 'YOUR_PROJECT_ID'  #@param {type: \"string\"}\n",
    "bucket = 'gs://YOUR_BUCKET_NAME-bucket' #@param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create your bucket, enable the video intelligence and AutoML APIs, and generate a service account key to access the AutoML API. For this lab, you will use your autogenerated Qwiklabs Service Account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QThWDprsbUCK"
   },
   "outputs": [],
   "source": [
    "!gcloud config set project {project_id}\n",
    "!gsutil mb {bucket}\n",
    "!gcloud iam service-accounts keys create ./key.json --iam-account {project_id}@{project_id}.iam.gserviceaccount.com\n",
    "\n",
    "# Enable the Video Intelligence API and AutoML\n",
    "!gcloud services enable videointelligence.googleapis.com\n",
    "!gcloud services enable automl.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ArhU6zUtWKsC"
   },
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./key.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCScPFGRT8cO"
   },
   "source": [
    "## Using the Video Intelligence API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IZIcR--7gkX6"
   },
   "source": [
    "In this section you will analyze a skeleton using the Video Intelligence API. Using Person Detection, the Video Intelligence API can detect the presence of humans in a video file and track the bounding box of individual people across the video or video segment. Person Detection recognizes a handful of body parts, facial features, and clothing as \"landmarks\". A table from the documentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://daleonai.com/images/screen-shot-2020-07-14-at-3-45-56-pm.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API tracks all sorts of features, including facial features. For this lab you will only focus on the body."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JVC1kv8ziVb3"
   },
   "source": [
    "![alt text](https://github.com/google/making_with_ml/blob/master/sports_ai/assets/posture_tracking.gif?raw=true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bKvXbYLTWsHr"
   },
   "source": [
    "To do this, you will use a prerecorded video of multiple tennis serves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7K8CE277Yerh"
   },
   "outputs": [],
   "source": [
    "input_uri = \"gs://spls/aiforsports/tennis_serves.mp4\"\n",
    "output_uri = os.path.join(bucket, 'output.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the Video Intelligence API's Person Detection feature on the video from the bucket. This function takes an input path to your video and an output path where you want your results stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bb-FvDOMWXLp"
   },
   "outputs": [],
   "source": [
    "# This function comes from the docs\n",
    "# https://cloud.google.com/video-intelligence/docs/people-detection\n",
    "def detect_person(input_uri, output_uri):\n",
    "    \"\"\"Detects people in a video.\"\"\"\n",
    "\n",
    "    client = videointelligence.VideoIntelligenceServiceClient(credentials=service_account.Credentials.from_service_account_file(\n",
    "    './key.json'))\n",
    "\n",
    "    # Configure the request\n",
    "    config = videointelligence.types.PersonDetectionConfig(\n",
    "        include_bounding_boxes=True,\n",
    "        include_attributes=True,\n",
    "        include_pose_landmarks=True,\n",
    "    )\n",
    "    context = videointelligence.types.VideoContext(person_detection_config=config)\n",
    "\n",
    "    # Start the asynchronous request\n",
    "    operation = client.annotate_video(\n",
    "        input_uri=input_uri,\n",
    "        output_uri=output_uri,\n",
    "        features=[videointelligence.enums.Feature.PERSON_DETECTION],\n",
    "        video_context=context,\n",
    "    )\n",
    "\n",
    "    return operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ED-SY53rY2SC"
   },
   "outputs": [],
   "source": [
    "operation = detect_person(input_uri, output_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EM2AyHZfcZ7V"
   },
   "source": [
    "`detect_person` is an asynchronous function and it takes a minute or two for the entire video to be analyzed. You can check the status of the analysis by calling `operation.done`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_So3U0wOcXTN"
   },
   "outputs": [],
   "source": [
    "print(f\"Operation ${operation.operation.name} is done? {operation.done()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eKV3eiI1ci2A"
   },
   "source": [
    "Note that even if you restart this notebook, the Video Intelligence API will still be analyzing your video in the cloud, so you won't lose any progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4aHITq4KbvXY"
   },
   "source": [
    "Rerun the previous command until it outputs `True`. Once the operation is finished, download the results from your Cloud Storage bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPfdGwF3b5Hi"
   },
   "outputs": [],
   "source": [
    "# Note! This won't work unless operation.done() == True!\n",
    "!mkdir tmp\n",
    "!gsutil cp {output_uri} tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MD5VJdgNfn3J"
   },
   "source": [
    "## Formatting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K_evMqtHdqMN"
   },
   "source": [
    "Results are written to Sloud Storage as a `json` file. Run this command to load them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4EBRYBldpAa"
   },
   "outputs": [],
   "source": [
    "data = json.load(open('./tmp/output.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDvqJ8XtdxuQ"
   },
   "source": [
    "These `json` files are usually pretty big, so don't print them! Instead, inspect the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ichZ8jCjdVXO",
    "outputId": "068c14cb-fca1-4a2e-9b06-f1f55cb0f9b6"
   },
   "outputs": [],
   "source": [
    "print(data.keys())\n",
    "# You only care about annotation_results[0] because you only have one video\n",
    "print(len(data['annotation_results'][0]['person_detection_annotations']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NdM9FkUReMiy"
   },
   "source": [
    "It's easy to get lost in all of these nested fields, but the important data for this lab is stored in `data['annotation_results'][0]['person_detection_annotations']`. Store it in `people_annotations`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ALNhgV0hd-DJ"
   },
   "outputs": [],
   "source": [
    "people_annotations = data['annotation_results'][0]['person_detection_annotations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IS-wxf-defDL"
   },
   "source": [
    "In `people_annotations`, every entry corresponds to a person, and each person has a unique set of `tracks`, or tracked segments. Write a helper function to parse through the data and rearrange it to make it easier to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNCq7v6edqmW"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This helper function takes in a person and rearranges the data so it's in \n",
    "a timeline, which will make it easier for us to work with\n",
    "'''\n",
    "def analyzePerson(person):\n",
    "  frames = []\n",
    "  for track in person['tracks']:\n",
    "    # Convert timestamps to seconds\n",
    "    for ts_obj in track['timestamped_objects']:\n",
    "      time_offset = ts_obj['time_offset']\n",
    "      timestamp = 0\n",
    "      if 'nanos' in time_offset:\n",
    "        timestamp += time_offset['nanos'] / 10**9\n",
    "      if 'seconds' in time_offset:\n",
    "        timestamp += time_offset['seconds']\n",
    "      if 'minutes' in time_offset:\n",
    "        timestamp += time_offset['minutes'] * 60\n",
    "      frame= {'timestamp' : timestamp}\n",
    "      for landmark in ts_obj['landmarks']:\n",
    "        frame[landmark['name'] + '_x'] = landmark['point']['x']\n",
    "        # Subtract y value from 1 because positions are calculated\n",
    "        # from the top left corner\n",
    "        frame[landmark['name'] + '_y'] = 1 - landmark['point']['y']\n",
    "      frames.append(frame)\n",
    "  sorted(frames, key=lambda x: x['timestamp'])\n",
    "  return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nr1ODUCdfGSL"
   },
   "source": [
    "Next, store the data in a pandas DataFrame (also for convenience), and sort each data point by timestamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "weH_f4-qb010"
   },
   "outputs": [],
   "source": [
    "annotationsPd = pd.DataFrame(analyzePerson(people_annotations[0]))\n",
    "for annotation in people_annotations[1:]:\n",
    "  annotationsPd = annotationsPd.append(pd.DataFrame(analyzePerson(annotation)))\n",
    "\n",
    "annotationsPd = annotationsPd.sort_values('timestamp', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W8HUTOKHfMB0"
   },
   "source": [
    "Now, take a look at the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "PfN0DEsJfPTU",
    "outputId": "b9637305-4b54-4acc-fe84-4a6007f5aea1"
   },
   "outputs": [],
   "source": [
    "annotationsPd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rWa-a3GpfTSm"
   },
   "source": [
    "As you can see above, the data is organized by the position of each body part by timestamp. Note: this works because there is only one person in the video used for this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vdii750DfkqH"
   },
   "source": [
    "## Plotting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HLAvIhNWfvYi"
   },
   "source": [
    "With the video data formatted, plot the positions of the wrists and try to determine the start and end time of a serve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "mNS6T8r6gnPN",
    "outputId": "7e3a7dab-2b9d-461f-86bd-11a1fa44aada"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "annotationsPd.plot('timestamp', ['left_wrist_y', 'right_wrist_y'], figsize=(20, 5))\n",
    "plt.title(\"Left and Right Wrist Positions Over Time\")\n",
    "plt.savefig(\"wrist_pos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FVZjAThPgGRz"
   },
   "source": [
    "From the plot above, you can actually identify the time of a serve pretty easily! First, the tennis ball is thrown up with the left hand (peak in left wrist/blue line). Then, a few seconds later, the ball is hit with the racket (peak in right wrist/orange line).\n",
    "\n",
    "The above plot is useful, but what would be even better would be understanding the angles of the elbow, knee, etc., in relation to the positions of the wrists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b4VBbs5o1BIJ"
   },
   "source": [
    "## Computing Angles\n",
    "Step 1: Create some classes for making working with points easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_IEcgmNn-Ge"
   },
   "outputs": [],
   "source": [
    "class Point:\n",
    "  def __init__(self, x, y):\n",
    "    self.x = x\n",
    "    self.y = y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TDjDF5Nmp3Li"
   },
   "source": [
    "To compute the angle made by three points, use the [Law of Cosines](https://www.google.com/search?q=rule+of+cosines&oq=rule+of+cosines&aqs=chrome.0.0l8.1048j1j7&sourceid=chrome&ie=UTF-8#wptab=s:H4sIAAAAAAAAAONgVuLQz9U3SMkuLnjE6Mgt8PLHPWEpi0lrTl5jNOLiCs7IL3fNK8ksqRRS4WKDsqS4eKTgmjQYpLi44DyeXUyCzvl5yakFJc6JOcmlOYkl-UWLWCV9EssV8tMUkvOLM_NSixWS4XIALyw914AAAAA). Imagine a triangle with side lengths a, b, and c. Then, to find 𝛾 (the angle across from side c), the formula is:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\gamma = \\cos^{-1}\\frac{a^2+b^2 - c^2}{2ab}\n",
    "\\end{equation*}\n",
    "\n",
    "The math is provided for you in this lab, but if it feels a bit confusing, there's a good explanation and code sample [here](https://medium.com/@manivannan_data/find-the-angle-between-three-points-from-2d-using-python-348c513e2cd), from which this function is borrowed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zqFEBYrew97U"
   },
   "outputs": [],
   "source": [
    "def getAngle(a, b, c):\n",
    "    ang = math.degrees(math.atan2(c.y-b.y, c.x-b.x) - math.atan2(a.y-b.y, a.x-b.x))\n",
    "    return ang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PHmvNXysro1s"
   },
   "source": [
    "Using the `point` class and `getAngle` method, compute some useful angles below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aVYS53rrtiSW"
   },
   "outputs": [],
   "source": [
    "def computeElbowAngle(row, which='right'):\n",
    "  wrist = Point(row[f'{which}_wrist_x'], row[f'{which}_wrist_y'])\n",
    "  elbow = Point(row[f'{which}_elbow_x'], row[f'{which}_elbow_y'])\n",
    "  shoulder = Point(row[f'{which}_shoulder_x'], row[f'{which}_shoulder_y'])\n",
    "  return getAngle(wrist, elbow, shoulder)\n",
    "\n",
    "def computeShoulderAngle(row, which='right'):\n",
    "  elbow = Point(row[f'{which}_elbow_x'], row[f'{which}_elbow_y'])\n",
    "  shoulder = Point(row[f'{which}_shoulder_x'], row[f'{which}_shoulder_y'])\n",
    "  hip = Point(row[f'{which}_hip_x'], row[f'{which}_hip_y'])\n",
    "  return getAngle(hip, shoulder, elbow)\n",
    "\n",
    "def computeKneeAngle(row, which='right'):\n",
    "  hip = Point(row[f'{which}_hip_x'], row[f'{which}_hip_y'])\n",
    "  knee = Point(row[f'{which}_knee_x'], row[f'{which}_knee_y'])\n",
    "  ankle = Point(row[f'{which}_ankle_x'], row[f'{which}_ankle_y'])\n",
    "  return getAngle(ankle, knee, hip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "vAwOTnG0sKpT",
    "outputId": "c786363f-03b8-4393-fcfc-1ff898fc67a8"
   },
   "outputs": [],
   "source": [
    "# For a single timeslot...\n",
    "row = annotationsPd.iloc[-1]\n",
    "print(\"Elbow angle: \" + str(computeElbowAngle(row)))\n",
    "print(\"Shoulder angle: \" + str(computeShoulderAngle(row)))\n",
    "print(\"Knee angle: \" + str(computeKneeAngle(row)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f2zinLwT0rOx"
   },
   "source": [
    "Now, plot those angles over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQoo7SxJrPk3"
   },
   "outputs": [],
   "source": [
    "annotationsPd['right_elbow_angle'] = annotationsPd.apply(computeElbowAngle, axis=1)\n",
    "annotationsPd['right_shoulder_angle'] = annotationsPd.apply(computeShoulderAngle, axis=1)\n",
    "annotationsPd['right_knee_angle'] = annotationsPd.apply(computeKneeAngle, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cyuIp_72gTTe"
   },
   "source": [
    "Next, plot the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6iaAPJJl-faG",
    "outputId": "0e13d8be-8f14-4f1c-b0d1-9106f1bd4851"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "annotationsPd.plot('timestamp', ['right_elbow_angle'], figsize=(20, 5), color='blue')\n",
    "plt.title(\"Right Elbow Angle over Time\")\n",
    "plt.savefig(\"right_elbow_angle\")\n",
    "annotationsPd.plot('timestamp', ['right_shoulder_angle'], figsize=(20, 5), color='purple')\n",
    "plt.title(\"Right Shoulder Angle over Time\")\n",
    "plt.savefig(\"right_shoulder_angle\")\n",
    "annotationsPd.plot('timestamp', ['right_knee_angle'], figsize=(20, 5))\n",
    "plt.title(\"Right Knee Angle over Time\")\n",
    "plt.savefig(\"right_knee_angle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1b3jJtdp3379"
   },
   "source": [
    "These angles might not be very useful on their own, but when combined with position data, you can tell what the angle of the arm was _at the height of the serve_. In particular, take a look at the angle of the elbow and shoulder when the right wrist was at the highest point in the serve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "LnWRGitk4BVy",
    "outputId": "53272d00-4311-41b7-a0aa-e4a2f24f51fc"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax=fig.add_subplot(111, label=\"1\")\n",
    "annotationsPd.plot('timestamp', ['right_wrist_y'], figsize=(20, 5), ax=ax, color='red')\n",
    "plt.title(\"Right Elbow Angle over Time\")\n",
    "\n",
    "ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "annotationsPd.plot('timestamp', ['right_elbow_angle'], figsize=(20, 5), ax=ax2)\n",
    "\n",
    "#annotationsPd.plot.scatter('right_wrist_y', 'right_elbow_angle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Vx7Dc7l6Tpv"
   },
   "source": [
    "These charts might be difficult to read, but but they show that the elbow is at a 200 degree angle when the arm is most extended. In an ideal tennis serve, the elbow would be straight when the arm is most extended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZtX6hAyTgfV-"
   },
   "source": [
    "# Tracking the Speed of the Ball With AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iJf1l3-Oiu9y"
   },
   "source": [
    "To compute the speed of the ball, you will use [AutoML Vision Object Detection](https://cloud.google.com/vision/automl/object-detection/docs). Training an AutoML Vision model can be a long process, so for this lab, you will send an authenticated Cloud Run request to a pretrained model.\n",
    "\n",
    "Check out the [blog post](https://daleonai.com/machine-learning-for-sports) if you are curious about how the model was trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yijp-ftZ7uAg"
   },
   "source": [
    "To make a prediction with AutoML, first convert the video of tennis serves into images. Download the video from the public bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "hFf-KUsF72lG",
    "outputId": "475e8043-c07b-4124-b14e-abfadfe846bf"
   },
   "outputs": [],
   "source": [
    "!gsutil cp gs://spls/aiforsports/tennis_serves.mp4 .\n",
    "filename = \"tennis_serves.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "52uXJ5a23fws"
   },
   "source": [
    "Use the `ffmpeg` command to generate snapshots from the tennis serve video at 20 frames per second. The `-t` flag of this command specifies a 2 second segment (`-t 00:00:02`) that starts from one second in (`-ss 00:00:01`). This should align with the first serve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "colab_type": "code",
    "id": "Tqr8fDt577qa",
    "outputId": "b7071740-6c2d-4fd5-c5fe-e0071c509ef4"
   },
   "outputs": [],
   "source": [
    "!mkdir tmp/snapshots\n",
    "!ffmpeg -i {filename} -vf fps=20 -ss 00:00:01 -t 00:00:02 tmp/snapshots/%03d.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of use, store the snapshots in a variable and sort them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshotFiles = os.listdir('tmp/snapshots')\n",
    "snapshotFiles.sort()\n",
    "print(f\"Analyzing {len(snapshotFiles)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, upload these snapshots back into your bucket so that they will be easy to use in the Cloud Run prediction request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_bucket_location = f\"{bucket}/snapshots\"\n",
    "!gsutil -m cp ./tmp/snapshots/* {snapshot_bucket_location}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write the prediction function which uses the AutoML model to analyze those snapshots. In order to do that, you will need a valid ID token to access the pretrained model.\n",
    "\n",
    "Access your student ID token by logging in to [this authentication page](https://gsp-auth-kjyo252taq-uc.a.run.app/) with your student email address and password."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once logged in click the __Copy__ button, paste it into this id_token variable and run the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_token = \"PASTE_YOUR_ID_TOKEN_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If you don't see an ID token after logging in, you probably need to allow cookies for that page in your browser. This will consistently happen if running in incognito mode. In Chrome, cookies can be enabled by clicking the eye icon to the far right of your URL tab.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you are able to analyze the snapshots. Write the `getAutoMLPrediction` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import re\n",
    "import logging\n",
    "import requests\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def getAutoMLPrediction():\n",
    "    responseList = []\n",
    "    match = re.match(r'gs://([^/]+)/(.+)', snapshot_bucket_location)\n",
    "    bucket_name = match.group(1)\n",
    "    prefix = match.group(2)\n",
    "    storage_client = storage.Client.from_service_account_json(\"./key.json\") \n",
    "    my_bucket = storage_client.bucket(bucket_name)\n",
    "    for blob in my_bucket.list_blobs(prefix=str(prefix + \"/\")):\n",
    "        if blob.name.endswith(\".jpg\"):\n",
    "            logger.info(\"File location: {}\".format(os.path.join('gs://',bucket_name, blob.name)))\n",
    "            params = {}\n",
    "            \n",
    "            #[START] GSP666-API REQUEST\n",
    "            url_lifetime = 3600  # Seconds in an hour\n",
    "            serving_url = blob.generate_signed_url(expiration=url_lifetime, version='v4')\n",
    "            # response = prediction_client.predict(model_full_id, payload, params)\n",
    "            header = {'Authorization': 'Bearer ' + id_token}\n",
    "            data = {\"image_url\" : serving_url,\n",
    "                \"compute_region\": \"us-central1\",\n",
    "                \"model_id\": \"IOD5928674449107714048\",\n",
    "                }\n",
    "            url = 'https://gsp666-api-kjyo252taq-uc.a.run.app/image'\n",
    "            response = requests.post(url, json=data, headers=header)\n",
    "            response_data = response.json()\n",
    "            logger.info(\"response data from API\")\n",
    "            logger.info(response_data)\n",
    "            #[END] GSP666-API REQUEST\n",
    "            if \"payload\" in  response_data:\n",
    "                responseList.append([obj['imageObjectDetection']['boundingBox']['normalizedVertices'] for obj in response_data['payload']])\n",
    "            else:\n",
    "                responseList.append({})\n",
    "            \n",
    "    return responseList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `getAutoMLPredicion` function loops through your snapshots and generates a payload (the data variable) from the snapshot's url within the bucket, the compute region (AutoML predict only supports us-central1 at the moment), and the model ID of the pretrained model. The function uses the ID token you stored earlier to authorize this Cloud Run request to the model.\n",
    "\n",
    "The response from the Cloud Run request is the response from the pretrained model that is exposed via Cloud Run. When you run the function, the logs will display the information in each response object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the __getAutoMLPrediciton__ function written, run it and assign its return value (the responses) to the coords variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the AutoML API--this could take a while!\n",
    "coords = getAutoMLPrediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't be alarmed by snapshots that return empty responses. If the prediction score of the model was too low, it will return an empty response. In this case, this is usually because there is no tennis ball in some of the snapshots but can also be the result of the small margin of error the model has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LXve_aPG3xft"
   },
   "source": [
    "Now with the AutoML data, create an image to see what's actually going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IkKlagxeiEem"
   },
   "outputs": [],
   "source": [
    "def makeBallImage(filename, coords):\n",
    "  im = Image.open(filename)\n",
    "  im.thumbnail((im.width * 0.2, im.height * 0.2))\n",
    "  draw = ImageDraw.Draw(im)\n",
    "  for coord in coords:\n",
    "    draw.rectangle([(coord[0]['x'] * im.width, coord[0]['y'] * im.height), coord[1]['x'] * im.width, coord[1]['y'] * im.height])\n",
    "  return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BCCI1xMgXEHw"
   },
   "outputs": [],
   "source": [
    "imgs = [makeBallImage('tmp/snapshots/' + filename, coord) for filename, coord in zip(snapshotFiles, coords) if 'jpg' in filename]\n",
    "!mkdir snapshot_annotated\n",
    "for idx, im in enumerate(imgs):\n",
    "  plt.imshow(np.asarray(im))\n",
    "  plt.savefig('snapshot_annotated/file%d.png' % idx)\n",
    "  \n",
    "# Create a cute video of your serves!\n",
    "!ffmpeg -framerate 20 -i snapshot_annotated/file%01d.png -vcodec mpeg4 -y ball_tracking.mp4\n",
    "!ffmpeg -i ball_tracking.mp4 ball_tracking.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TSA8-oMV34Iq"
   },
   "source": [
    "The code above analyzes the snapshots and creates a gif and video you can check out in the files `ball_tracking.mp4` and `ball_tracking.gif` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IBbWimItvTUU"
   },
   "source": [
    "Now that the ball is tracked, you can compute it's position and then speed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X9qxyEYDvWSs"
   },
   "outputs": [],
   "source": [
    "# For simplicity, just plot the bottom left corner of the bounding box\n",
    "# around the ball\n",
    "coord_x = [ball[0]['x'] for frame in coords for ball in frame]\n",
    "coord_y = [1 - ball[0]['y'] for frame in coords for ball in frame]\n",
    "timestamps = [x/20 for x in range(len(coord_x))] # 20 frames per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UB6sQnEbzkk9"
   },
   "source": [
    "Plot the ball in space, and see how it leaves the hand and then flies across the court."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "FK4kunlDzjWp",
    "outputId": "9cc651c1-ab20-4d23-a7c7-344f299dc9c5"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Position of tennis ball during serve\")\n",
    "plt.xlabel(\"X Position\")\n",
    "plt.ylabel(\"Y Position\")\n",
    "plt.scatter(coord_x, coord_y)\n",
    "plt.savefig(\"serve_position_x_y.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AJquxCBH1Mwn"
   },
   "source": [
    "To determine the speed, look at the distance the ball travels over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "DXNlQanq0cJT",
    "outputId": "218d8c15-59bb-4b9f-dc60-df67082a00e2"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Y position of tennis ball during serve over time\")\n",
    "plt.xlabel(\"seconds\")\n",
    "plt.ylabel(\"Y position\")\n",
    "plt.scatter(timestamps, coord_y)\n",
    "plt.savefig(\"ball_position_over_time.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mcb_8kkh1La6"
   },
   "source": [
    "You can see that 0.5 to 0.7 seconds is when the ball has been hit and is traveling across the court. So, to compute the speed, divide distance by time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_QJIocMx1k2l",
    "outputId": "09216d49-fa7e-4812-d82e-a026f7e06a1a"
   },
   "outputs": [],
   "source": [
    "# Get the first data point from 0.5 seconds\n",
    "start_x = coord_x[timestamps.index(0.5)]\n",
    "end_x = coord_x[-1]\n",
    "start_y = coord_y[timestamps.index(0.5)]\n",
    "end_y = coord_y[-1]\n",
    "\n",
    "# Compute the Euclidean distance\n",
    "distance = math.sqrt((start_x - end_x)**2 + (start_y - end_y)**2)\n",
    "time = timestamps[-1] - 0.5\n",
    "\n",
    "print(f\"The speed of your serve was {distance/time}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOqGhxmJCNuwJPtF75Ha16V",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Sports AI Analysis",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m54"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
